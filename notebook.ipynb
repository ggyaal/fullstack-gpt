{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692c8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  temperature=0.1,\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "  separator=\"\\n\",\n",
    "  chunk_size=600,\n",
    "  chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "  embeddings, cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\n",
    "      \"system\",\n",
    "      \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "  ]\n",
    ")\n",
    "\n",
    "def load_memory(_):\n",
    "  return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = (\n",
    "  {\n",
    "    \"context\": retriver,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"history\": RunnableLambda(load_memory),\n",
    "  }\n",
    "  | prompt\n",
    "  | llm\n",
    ")\n",
    "\n",
    "def invoke_chain(question):\n",
    "  result = chain.invoke(question)\n",
    "  memory.save_context(\n",
    "    {\"input\": question},\n",
    "    {\"output\": result.content},\n",
    "  )\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3133e0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='제가 알기로는 Aaronson이 유죄임을 확실히 알 수 없습니다.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Aaronson은 유죄인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44886bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='그가 쓴 메시지는 다음과 같습니다:\\nFREEDOM IS SLAVERY\\nTWO AND TWO MAKE FIVE\\nGOD IS POWER'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"그가 테이블에 어떤 메세지를 썼나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e9608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Julia는 주인공과 함께 이야기하는 여성 캐릭터입니다.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Julia는 누구인가요?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
